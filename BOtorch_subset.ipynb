{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilkenMocha/Hackathon_Ac_BO/blob/main/BOtorch_subset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch_geometric\n",
        "!pip install ax_platform\n",
        "!pip install botorch"
      ],
      "metadata": {
        "id": "0EvEOiiTEJxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvW_LJG7EBKW"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import GRU, Linear, ReLU, Sequential\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import NNConv, Set2Set\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "import pandas as pd\n",
        "\n",
        "target = 0\n",
        "dim = 64\n",
        "\n",
        "\n",
        "class MyTransform:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        data.y = data.y[:, target]  # Specify target.\n",
        "        return data\n",
        "\n",
        "\n",
        "class Complete:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "path = osp.join(osp.dirname(osp.realpath(\"__file__\")), '..', 'data', 'QM9')\n",
        "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
        "# Originalmente: Carga y preparación del dataset\n",
        "dataset = QM9(path, transform=transform).shuffle()\n",
        "\n",
        "# Modificación para usar un subconjunto más pequeño\n",
        "# Por ejemplo, usar solo los primeros 10,000 datos del dataset\n",
        "\n",
        "subset_size = 1000  # Define el tamaño del subconjunto\n",
        "subset_indices = torch.randperm(len(dataset), generator=torch.Generator().manual_seed(15))[:subset_size]  # Selecciona índices al azar\n",
        "dataset = dataset[subset_indices]\n",
        "\n",
        "# Procede como antes\n",
        "mean = dataset.data.y.mean(dim=0, keepdim=True)\n",
        "std = dataset.data.y.std(dim=0, keepdim=True)\n",
        "dataset.data.y = (dataset.data.y - mean) / std\n",
        "mean, std = mean[:, target].item(), std[:, target].item()\n",
        "\n",
        "# Split datasets teniendo en cuenta el nuevo tamaño\n",
        "test_dataset = dataset[:int(subset_size*0.2)]\n",
        "val_dataset = dataset[int(subset_size*0.2):int(subset_size*0.4)]\n",
        "train_dataset = dataset[int(subset_size*0.4):]\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "def objective_function(dim_values):\n",
        "    # Crear un DataFrame para almacenar los resultados\n",
        "    results = pd.DataFrame(columns=['Dim', 'Función de pérdida'])\n",
        "\n",
        "    # Iterar sobre los valores de neuronas\n",
        "    for dim in dim_values:\n",
        "        # Redefinir la arquitectura de la red neuronal con el nuevo número de neuronas\n",
        "        class Net(torch.nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
        "\n",
        "                nn = Sequential(Linear(5, 128), ReLU(), Linear(128, dim * dim))\n",
        "                self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
        "                self.gru = GRU(dim, dim)\n",
        "\n",
        "                self.set2set = Set2Set(dim, processing_steps=3)\n",
        "                self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
        "                self.lin2 = torch.nn.Linear(dim, 1)\n",
        "\n",
        "            # Propagación de información\n",
        "            def forward(self, data):\n",
        "                out = F.relu(self.lin0(data.x))\n",
        "                h = out.unsqueeze(0)\n",
        "\n",
        "                for i in range(3):\n",
        "                    m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
        "                    out, h = self.gru(m.unsqueeze(0), h)\n",
        "                    out = out.squeeze(0)\n",
        "\n",
        "                out = self.set2set(out, data.batch)\n",
        "                out = F.relu(self.lin1(out))\n",
        "                out = self.lin2(out)\n",
        "                return out.view(-1)\n",
        "\n",
        "        # Función de pérdida\n",
        "        torch.cuda.manual_seed(15)\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = Net().to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                               factor=0.7, patience=5,\n",
        "                                                               min_lr=0.00001)  # Ajusta el LR durante el entrenamiento.\n",
        "\n",
        "        # Entrenamiento de red neuronal\n",
        "        def train(epoch):\n",
        "            model.train()\n",
        "            loss_all = 0\n",
        "\n",
        "            for data in train_loader:\n",
        "                data = data.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                loss = F.mse_loss(model(data), data.y)\n",
        "                loss.backward()\n",
        "                loss_all += loss.item() * data.num_graphs\n",
        "                optimizer.step()\n",
        "            return loss_all / len(train_loader.dataset)\n",
        "\n",
        "        # Test de red neuronal\n",
        "        def test(loader):\n",
        "            model.eval()\n",
        "            error = 0\n",
        "\n",
        "            for data in loader:\n",
        "                data = data.to(device)\n",
        "                error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
        "            return error / len(loader.dataset)\n",
        "\n",
        "        # Entrenamiento del modelo en épocas\n",
        "        best_val_error = None\n",
        "        for epoch in range(1, 51):\n",
        "            lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "            loss = train(epoch)\n",
        "            val_error = test(val_loader)\n",
        "            scheduler.step(val_error)\n",
        "\n",
        "            if best_val_error is None or val_error <= best_val_error:\n",
        "                test_error = test(test_loader)\n",
        "                best_val_error = val_error\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
        "                  f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')\n",
        "        # Agregar los resultados al DataFrame\n",
        "        results = results.append({'Dim': dim, 'Función de pérdida': best_val_error}, ignore_index=True)\n",
        "\n",
        "    # Imprimir los resultados\n",
        "    print(results)\n",
        "\n",
        "# Definir los valores de neuronas para probar\n",
        "dim_values = [64, 128, 256, 512]\n",
        "\n",
        "# Llamar a la función objetivo\n",
        "objective_function(dim_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import GRU, Linear, ReLU, Sequential\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import NNConv, Set2Set\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "import pandas as pd\n",
        "\n",
        "target = 0\n",
        "dim = 64\n",
        "\n",
        "\n",
        "class MyTransform:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        data.y = data.y[:, target]  # Specify target.\n",
        "        return data\n",
        "\n",
        "\n",
        "class Complete:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "path = osp.join(osp.dirname(osp.realpath(\"__file__\")), '..', 'data', 'QM9')\n",
        "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
        "# Originalmente: Carga y preparación del dataset\n",
        "dataset = QM9(path, transform=transform).shuffle()\n",
        "\n",
        "# Modificación para usar un subconjunto más pequeño\n",
        "# Por ejemplo, usar solo los primeros 10,000 datos del dataset\n",
        "\n",
        "\n",
        "def subset_size_objective(subset_size):\n",
        "    # Define el tamaño del subconjunto\n",
        "    subset_indices = torch.randperm(len(dataset), generator=torch.Generator().manual_seed(15))[:subset_size]\n",
        "    subset_dataset = dataset[subset_indices]\n",
        "\n",
        "    # Calcula la media y la desviación estándar de los datos y normaliza las etiquetas\n",
        "    mean = subset_dataset.data.y.mean(dim=0, keepdim=True)\n",
        "    std = subset_dataset.data.y.std(dim=0, keepdim=True)\n",
        "    subset_dataset.data.y = (subset_dataset.data.y - mean) / std\n",
        "    mean, std = mean[:, target].item(), std[:, target].item()\n",
        "\n",
        "    # Divide el conjunto de datos en entrenamiento, validación y prueba\n",
        "    test_dataset = subset_dataset[:int(subset_size*0.2)]\n",
        "    val_dataset = subset_dataset[int(subset_size*0.2):int(subset_size*0.4)]\n",
        "    train_dataset = subset_dataset[int(subset_size*0.4):]\n",
        "\n",
        "    # Carga los datos en los DataLoaders\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    # Define la arquitectura de la red neuronal\n",
        "    class Net(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
        "\n",
        "            nn = Sequential(Linear(5, 128), ReLU(), Linear(128, dim * dim))\n",
        "            self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
        "            self.gru = GRU(dim, dim)\n",
        "\n",
        "            self.set2set = Set2Set(dim, processing_steps=3)\n",
        "            self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
        "            self.lin2 = torch.nn.Linear(dim, 1)\n",
        "\n",
        "        def forward(self, data):\n",
        "            out = F.relu(self.lin0(data.x))\n",
        "            h = out.unsqueeze(0)\n",
        "\n",
        "            for i in range(3):\n",
        "                m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
        "                out, h = self.gru(m.unsqueeze(0), h)\n",
        "                out = out.squeeze(0)\n",
        "\n",
        "            out = self.set2set(out, data.batch)\n",
        "            out = F.relu(self.lin1(out))\n",
        "            out = self.lin2(out)\n",
        "            return out.view(-1)\n",
        "\n",
        "    # Inicializa el modelo, optimizador y programador de tasa de aprendizaje\n",
        "    torch.cuda.manual_seed(15)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net().to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                           factor=0.7, patience=5,\n",
        "                                                           min_lr=0.00001)\n",
        "\n",
        "    #Entrenamiento de red neuronal\n",
        "    def train(epoch):\n",
        "        model.train()\n",
        "        loss_all = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.mse_loss(model(data), data.y)\n",
        "            loss.backward()\n",
        "            loss_all += loss.item() * data.num_graphs\n",
        "            optimizer.step()\n",
        "        return loss_all / len(train_loader.dataset)\n",
        "\n",
        "    #Test de red neuronal\n",
        "    def test(loader):\n",
        "        model.eval()\n",
        "        error = 0\n",
        "\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
        "        return error / len(loader.dataset)\n",
        "\n",
        "\n",
        "    # Entrena el modelo y evalúa el rendimiento\n",
        "    best_val_error = None\n",
        "    for epoch in range(1, 6):\n",
        "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "        loss = train(epoch)\n",
        "        val_error = test(val_loader)\n",
        "        scheduler.step(val_error)\n",
        "\n",
        "        if best_val_error is None or val_error <= best_val_error:\n",
        "            test_error = test(test_loader)\n",
        "            best_val_error = val_error\n",
        "\n",
        "        print(f'Epoch: {epoch:03d}, Batch: {size.item()}, LR: {lr:7f}, Loss: {loss:.7f}, '\n",
        "              f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')\n",
        "\n",
        "    # Devuelve la pérdida final\n",
        "    return best_val_error\n",
        "\n",
        "# Tamaño del subconjunto a optimizar\n",
        "subset_size = [500, 750, 1000, 1250]  # Ejemplo de tamaños de subconjunto\n",
        "\n",
        "# Itera sobre los tamaños del subconjunto y calcula la pérdida final\n",
        "for size in subset_size:\n",
        "    subset_loss = subset_size_objective(size)\n",
        "    print(f'Subset Size: {size.item()}, Final Loss: {subset_loss}')\n"
      ],
      "metadata": {
        "id": "CCdThfkhQpin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bayesian Optimization for subset sizes"
      ],
      "metadata": {
        "id": "x2YWFvfsF_Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import GRU, Linear, ReLU, Sequential\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import NNConv, Set2Set\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from botorch.fit import fit_gpytorch_model\n",
        "from botorch.acquisition import ExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "\n",
        "target = 0\n",
        "dim = 64\n",
        "\n",
        "\n",
        "class MyTransform:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        data.y = data.y[:, target]  # Specify target.\n",
        "        return data\n",
        "\n",
        "\n",
        "class Complete:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "path = osp.join(osp.dirname(osp.realpath(\"__file__\")), '..', 'data', 'QM9')\n",
        "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
        "# Originalmente: Carga y preparación del dataset\n",
        "dataset = QM9(path, transform=transform).shuffle()\n",
        "\n",
        "# Modificación para usar un subconjunto más pequeño\n",
        "# Por ejemplo, usar solo los primeros 10,000 datos del dataset\n",
        "\n",
        "\n",
        "def subset_size_objective(subset_size):\n",
        "    # Define el tamaño del subconjunto\n",
        "    subset_indices = torch.randperm(len(dataset), generator=torch.Generator().manual_seed(15))[:subset_size]\n",
        "    subset_dataset = dataset[subset_indices]\n",
        "\n",
        "    # Calcula la media y la desviación estándar de los datos y normaliza las etiquetas\n",
        "    mean = subset_dataset.data.y.mean(dim=0, keepdim=True)\n",
        "    std = subset_dataset.data.y.std(dim=0, keepdim=True)\n",
        "    subset_dataset.data.y = (subset_dataset.data.y - mean) / std\n",
        "    mean, std = mean[:, target].item(), std[:, target].item()\n",
        "\n",
        "    # Divide el conjunto de datos en entrenamiento, validación y prueba\n",
        "    test_dataset = subset_dataset[:int(subset_size*0.2)]\n",
        "    val_dataset = subset_dataset[int(subset_size*0.2):int(subset_size*0.4)]\n",
        "    train_dataset = subset_dataset[int(subset_size*0.4):]\n",
        "\n",
        "    # Carga los datos en los DataLoaders\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    # Define la arquitectura de la red neuronal\n",
        "    class Net(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
        "\n",
        "            nn = Sequential(Linear(5, 128), ReLU(), Linear(128, dim * dim))\n",
        "            self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
        "            self.gru = GRU(dim, dim)\n",
        "\n",
        "            self.set2set = Set2Set(dim, processing_steps=3)\n",
        "            self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
        "            self.lin2 = torch.nn.Linear(dim, 1)\n",
        "\n",
        "        def forward(self, data):\n",
        "            out = F.relu(self.lin0(data.x))\n",
        "            h = out.unsqueeze(0)\n",
        "\n",
        "            for i in range(3):\n",
        "                m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
        "                out, h = self.gru(m.unsqueeze(0), h)\n",
        "                out = out.squeeze(0)\n",
        "\n",
        "            out = self.set2set(out, data.batch)\n",
        "            out = F.relu(self.lin1(out))\n",
        "            out = self.lin2(out)\n",
        "            return out.view(-1)\n",
        "\n",
        "    # Inicializa el modelo, optimizador y programador de tasa de aprendizaje\n",
        "    torch.cuda.manual_seed(15)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net().to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                           factor=0.7, patience=5,\n",
        "                                                           min_lr=0.00001)\n",
        "\n",
        "    #Entrenamiento de red neuronal\n",
        "    def train(epoch):\n",
        "        model.train()\n",
        "        loss_all = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.mse_loss(model(data), data.y)\n",
        "            loss.backward()\n",
        "            loss_all += loss.item() * data.num_graphs\n",
        "            optimizer.step()\n",
        "        return loss_all / len(train_loader.dataset)\n",
        "\n",
        "    #Test de red neuronal\n",
        "    def test(loader):\n",
        "        model.eval()\n",
        "        error = 0\n",
        "\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
        "        return error / len(loader.dataset)\n",
        "\n",
        "\n",
        "    # Entrena el modelo y evalúa el rendimiento\n",
        "    best_val_error = None\n",
        "    for epoch in range(1, 6):\n",
        "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "        loss = train(epoch)\n",
        "        val_error = test(val_loader)\n",
        "        scheduler.step(val_error)\n",
        "\n",
        "        if best_val_error is None or val_error <= best_val_error:\n",
        "            test_error = test(test_loader)\n",
        "            best_val_error = val_error\n",
        "\n",
        "        print(f'Epoch: {epoch:03d}, Batch:{subset_size} , LR: {lr:7f}, Loss: {loss:.7f}, '\n",
        "              f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')\n",
        "\n",
        "    # Devuelve la pérdida final\n",
        "    return best_val_error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Definir el espacio de búsqueda (los tamaños de subconjunto)\n",
        "subset_size = torch.tensor([[500], [750], [1000], [1250]])\n",
        "subset_sizef = subset_size.float()\n",
        "\n",
        "# Normalizar el espacio de búsqueda\n",
        "subset_size_normalized = (subset_sizef - subset_sizef.mean()) / subset_sizef.std()\n",
        "\n",
        "# Crear un modelo de regresión gaussiano para la optimización bayesiana\n",
        "train_X = subset_size_normalized\n",
        "train_Y = torch.tensor([subset_size_objective(size.item()) for size in subset_size]).unsqueeze(-1)\n",
        "gp = SingleTaskGP(train_X, train_Y)\n",
        "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "fit_gpytorch_model(mll)\n",
        "\n",
        "# Definir la adquisición (Expected Improvement)\n",
        "EI = ExpectedImprovement(gp, train_Y.min())\n",
        "\n",
        "# Realizar la optimización bayesiana\n",
        "candidate, acq_value = optimize_acqf(\n",
        "    EI,\n",
        "    bounds=torch.tensor([[-3.0], [3.0]]),  # Limitar la búsqueda al rango [-3, 3]\n",
        "    q=1,  # Número de puntos para la optimización multiarranque\n",
        "    num_restarts=20,  # Número de reinicios aleatorios para la optimización multiarranque\n",
        "    raw_samples=512,  # Número de muestras aleatorias para la optimización multiarranque\n",
        ")\n",
        "\n",
        "# Desnormalizar el candidato encontrado\n",
        "best_size = candidate * subset_size.float().std() + subset_size.float().mean() #Es necesario normalizar?\n",
        "\n",
        "print(f\"El mejor tamaño de subconjunto encontrado es: {best_size.item()}\")\n"
      ],
      "metadata": {
        "id": "ui9uyuQbPcDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bayesian Optimization within ranges of 500/1250 batch size"
      ],
      "metadata": {
        "id": "IZDqBiBRuvma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import GRU, Linear, ReLU, Sequential\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import NNConv, Set2Set\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from botorch.fit import fit_gpytorch_model\n",
        "from botorch.acquisition import ExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.utils import draw_sobol_samples\n",
        "\n",
        "\n",
        "target = 0\n",
        "dim = 64\n",
        "\n",
        "\n",
        "class MyTransform:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        data.y = data.y[:, target]  # Specify target.\n",
        "        return data\n",
        "\n",
        "\n",
        "class Complete:\n",
        "    def __call__(self, data):\n",
        "        data = copy.copy(data)\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "path = osp.join(osp.dirname(osp.realpath(\"__file__\")), '..', 'data', 'QM9')\n",
        "transform = T.Compose([MyTransform(), Complete(), T.Distance(norm=False)])\n",
        "# Originalmente: Carga y preparación del dataset\n",
        "dataset = QM9(path, transform=transform).shuffle()\n",
        "\n",
        "# Modificación para usar un subconjunto más pequeño\n",
        "# Por ejemplo, usar solo los primeros 10,000 datos del dataset\n",
        "\n",
        "\n",
        "def subset_size_objective(subset_sizes_samples):\n",
        "\n",
        "    # Define el tamaño del subconjunto\n",
        "    subset_indices = torch.randperm(len(dataset), generator=torch.Generator().manual_seed(15))[:subset_sizes_samples]\n",
        "    subset_dataset = dataset[subset_indices]\n",
        "\n",
        "    # Calcula la media y la desviación estándar de los datos y normaliza las etiquetas\n",
        "    mean = subset_dataset.data.y.mean(dim=0, keepdim=True)\n",
        "    std = subset_dataset.data.y.std(dim=0, keepdim=True)\n",
        "    subset_dataset.data.y = (subset_dataset.data.y - mean) / std\n",
        "    mean, std = mean[:, target].item(), std[:, target].item()\n",
        "\n",
        "    # Divide el conjunto de datos en entrenamiento, validación y prueba\n",
        "    test_dataset = subset_dataset[:int(subset_sizes_samples*0.2)]\n",
        "    val_dataset = subset_dataset[int(subset_sizes_samples*0.2):int(subset_sizes_samples*0.4)]\n",
        "    train_dataset = subset_dataset[int(subset_sizes_samples*0.4):]\n",
        "\n",
        "    # Carga los datos en los DataLoaders\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    # Define la arquitectura de la red neuronal\n",
        "    class Net(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
        "\n",
        "            nn = Sequential(Linear(5, 128), ReLU(), Linear(128, dim * dim))\n",
        "            self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
        "            self.gru = GRU(dim, dim)\n",
        "\n",
        "            self.set2set = Set2Set(dim, processing_steps=3)\n",
        "            self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
        "            self.lin2 = torch.nn.Linear(dim, 1)\n",
        "\n",
        "        def forward(self, data):\n",
        "            out = F.relu(self.lin0(data.x))\n",
        "            h = out.unsqueeze(0)\n",
        "\n",
        "            for i in range(3):\n",
        "                m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
        "                out, h = self.gru(m.unsqueeze(0), h)\n",
        "                out = out.squeeze(0)\n",
        "\n",
        "            out = self.set2set(out, data.batch)\n",
        "            out = F.relu(self.lin1(out))\n",
        "            out = self.lin2(out)\n",
        "            return out.view(-1)\n",
        "\n",
        "    # Inicializa el modelo, optimizador y programador de tasa de aprendizaje\n",
        "    torch.cuda.manual_seed(15)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net().to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                           factor=0.7, patience=5,\n",
        "                                                           min_lr=0.00001)\n",
        "\n",
        "    #Entrenamiento de red neuronal\n",
        "    def train(epoch):\n",
        "        model.train()\n",
        "        loss_all = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.mse_loss(model(data), data.y)\n",
        "            loss.backward()\n",
        "            loss_all += loss.item() * data.num_graphs\n",
        "            optimizer.step()\n",
        "        return loss_all / len(train_loader.dataset)\n",
        "\n",
        "    #Test de red neuronal\n",
        "    def test(loader):\n",
        "        model.eval()\n",
        "        error = 0\n",
        "\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
        "        return error / len(loader.dataset)\n",
        "\n",
        "\n",
        "    # Entrena el modelo y evalúa el rendimiento\n",
        "    best_val_error = None\n",
        "    for epoch in range(1, 6):\n",
        "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "        loss = train(epoch)\n",
        "        val_error = test(val_loader)\n",
        "        scheduler.step(val_error)\n",
        "\n",
        "        if best_val_error is None or val_error <= best_val_error:\n",
        "            test_error = test(test_loader)\n",
        "            best_val_error = val_error\n",
        "\n",
        "        print(f'Epoch: {epoch:03d}, Batch:{subset_sizes_samples} , LR: {lr:7f}, Loss: {loss:.7f}, '\n",
        "              f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')\n",
        "\n",
        "    # Devuelve la pérdida final\n",
        "    return best_val_error\n",
        "\n",
        "\n",
        "# Definir los límites inferior y superior para el tamaño del subconjunto\n",
        "subset_sizes_min = 500\n",
        "subset_sizes_max = 1250\n",
        "\n",
        "# Definir la dimensión de la entrada para el espacio de búsqueda\n",
        "num_samples = 4  # Número de muestras para la optimización bayesiana\n",
        "\n",
        "# Generar muestras sobol dentro del rango especificado para el tamaño del subconjunto\n",
        "subset_sizes_samples = torch.randint(subset_sizes_min, subset_sizes_max + 1, (num_samples, 1))\n",
        "\n",
        "# Normalizar las muestras si es necesario\n",
        "subset_sizes_normalized = (subset_sizes_samples.float() - subset_sizes_samples.float().mean()) / subset_sizes_samples.float().std()\n",
        "\n",
        "# Crear un modelo de regresión gaussiano para la optimización bayesiana\n",
        "train_X = subset_sizes_normalized\n",
        "train_Y = torch.tensor([subset_size_objective(size.item()) for size in subset_sizes_samples]).unsqueeze(-1)\n",
        "\n",
        "gp = SingleTaskGP(train_X, train_Y)\n",
        "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "fit_gpytorch_model(mll)\n",
        "\n",
        "# Definir la adquisición (Expected Improvement)\n",
        "EI = ExpectedImprovement(gp, train_Y.min())\n",
        "\n",
        "# Realizar la optimización bayesiana\n",
        "candidate, acq_value = optimize_acqf(\n",
        "    EI,\n",
        "    bounds=torch.tensor([[-3.0], [3.0]]),  # Limitar la búsqueda al rango [-3, 3]\n",
        "    q=1,  # Número de puntos para la optimización multiarranque\n",
        "    num_restarts=20,  # Número de reinicios aleatorios para la optimización multiarranque\n",
        "    raw_samples=512,  # Número de muestras aleatorias para la optimización multiarranque\n",
        ")\n",
        "\n",
        "# Desnormalizar el candidato encontrado\n",
        "best_size = candidate * subset_sizes_samples.float().std() + subset_sizes_samples.float().mean()\n",
        "\n",
        "print(f\"El mejor tamaño de subconjunto encontrado es: {best_size.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xM2QEI93PEnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}